{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-2:  Programming Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'S' b'F' b'F' b'F']\n",
      "[b'F' b'H' b'F' b'H']\n",
      "[b'F' b'F' b'F' b'H']\n",
      "[b'H' b'F' b'F' b'G']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pradeep/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.desc to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.desc` for environment variables or `env.get_wrapper_attr('desc')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "\n",
    "# Define the maximum number of epochs\n",
    "max_epochs = 500\n",
    "\n",
    "# Define a function to print the environment layout\n",
    "def print_env(env):\n",
    "    if hasattr(env, 'desc'):\n",
    "        for row in env.unwrapped.desc:\n",
    "            print(row)\n",
    "\n",
    "# Print the environment layout\n",
    "print_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.95, theta=1e-6):\n",
    "    # Get the number of states and actions from the environment\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the value function V to zeros for each state\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    # Loop for a maximum of 1000 epochs (iterations)\n",
    "    for epoch in range(1000):\n",
    "        # Initialize the maximum change in value function to zero\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate over all states in the environment\n",
    "        for s in range(num_states):\n",
    "            v = V[s]  # Store the current value for state s\n",
    "            q_values = np.zeros(num_actions)  # Initialize an array to store Q-values for each action\n",
    "\n",
    "            # Iterate over all possible actions\n",
    "            for a in range(num_actions):\n",
    "                # Iterate over possible outcomes of taking action a in state s\n",
    "                for prob, next_state, reward, _ in env.unwrapped.P[s][a]:\n",
    "                    # Update the Q-value for action a in state s using the Bellman equation\n",
    "                    q_values[a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "            # Update the value function for state s with the maximum Q-value\n",
    "            V[s] = max(q_values)\n",
    "\n",
    "            # Update the maximum change in value function\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        # If the maximum change in value function is below the specified threshold (theta), break the loop\n",
    "        if delta < theta:\n",
    "            print(f'Breaking at {epoch} epoch')\n",
    "            break\n",
    "\n",
    "    # Return the computed value function V\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at 6 epoch\n",
      "Value Function:\n",
      "[[0.77378094 0.81450625 0.857375   0.81450625]\n",
      " [0.81450625 0.         0.9025     0.        ]\n",
      " [0.857375   0.9025     0.95       0.        ]\n",
      " [0.         0.95       1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V_value_iter = value_iteration(env)\n",
    "\n",
    "print('Value Function:')\n",
    "print(V_value_iter.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.95):\n",
    "    # Get the number of states and actions from the environment\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the policy as an array of zeros\n",
    "    policy = np.zeros(num_states)\n",
    "\n",
    "    for s in range(num_states):\n",
    "        q_values = np.zeros(num_actions)\n",
    "\n",
    "        for a in range(num_actions):\n",
    "            # Iterate over possible outcomes of taking action a in state s\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[s][a]:\n",
    "                # Calculate the expected future reward using the value function V\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "        # Choose the action that maximizes the expected future reward\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.95, theta=1e-6):\n",
    "    # Get the number of states from the environment\n",
    "    num_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the value function V to zeros for each state\n",
    "    V = np.zeros(num_states)\n",
    "    \n",
    "    # Loop for a maximum of 1000 epochs (iterations)\n",
    "    for epoch in range(1000):\n",
    "        # Initialize the maximum change in value function to zero\n",
    "        delta = 0\n",
    "\n",
    "        # Iterate over all states in the environment\n",
    "        for s in range(num_states):\n",
    "            v = V[s]  # Store the current value for state s\n",
    "            new_v = 0  # Initialize a variable to store the updated value for state s\n",
    "\n",
    "            # Get the action to be taken in state s according to the policy\n",
    "            a = policy[s]\n",
    "\n",
    "            # Iterate over possible outcomes of taking action a in state s\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[s][a]:\n",
    "                # Update the new value for state s using the Bellman equation and the policy\n",
    "                new_v += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "            # Update the value function for state s with the new value\n",
    "            V[s] = new_v\n",
    "\n",
    "            # Update the maximum change in value function\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        # If the maximum change in value function is below the specified threshold (theta), break the loop\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Return the computed value function V\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_iteration(env, max_epochs=1000):\n",
    "    # Get the number of states and actions from the environment\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the value function V and the policy as arrays of zeros\n",
    "    V = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Policy Evaluation: Compute the value function V for the current policy\n",
    "        V = policy_evaluation(env, policy)\n",
    "\n",
    "        # Policy Improvement: Find an improved policy based on the current value function\n",
    "        new_policy = policy_improvement(env, V)\n",
    "\n",
    "        # If the policy has not changed, break the loop (the optimal policy has been found)\n",
    "        if (policy == new_policy).all():\n",
    "            print(f'Breaking at epoch: {epoch}')\n",
    "            break\n",
    "\n",
    "        # Update the current policy with the new policy\n",
    "        policy = new_policy\n",
    "\n",
    "    # Return the computed value function V and the optimal policy\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 6\n",
      "Environment Grid:\n",
      "[b'S' b'F' b'F' b'F']\n",
      "[b'F' b'H' b'F' b'H']\n",
      "[b'F' b'F' b'F' b'H']\n",
      "[b'H' b'F' b'F' b'G']\n",
      "\n",
      "Value Function:\n",
      " [[0.77378094 0.81450625 0.857375   0.81450625]\n",
      " [0.81450625 0.         0.9025     0.        ]\n",
      " [0.857375   0.9025     0.95       0.        ]\n",
      " [0.         0.95       1.         0.        ]]\n",
      "\n",
      "Policy:\n",
      " [[1. 2. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [2. 1. 1. 0.]\n",
      " [0. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "V_policy_iter, policy_policy_iter = policy_iteration(env)\n",
    "\n",
    "print('Environment Grid:')\n",
    "print_env(env)\n",
    "\n",
    "print('\\nValue Function:\\n', V_policy_iter.reshape(4,4))\n",
    "print('\\nPolicy:\\n', policy_policy_iter.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `render` function\n",
    "def render(policy, env, fname):\n",
    "    state, prob = env.reset()\n",
    "    images = []\n",
    "\n",
    "    while True:\n",
    "        images.append(env.render())\n",
    "        action = int(policy[state])\n",
    "        state, reward, done, info, prob = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    imageio.mimsave(fname, images, fps=10)\n",
    "\n",
    "# Create the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "\n",
    "policy_value_iter = policy_improvement(env, V_value_iter)\n",
    "\n",
    "# Render and save animations for both policies\n",
    "render(policy_policy_iter, env, 'snap_policy_iter.gif')\n",
    "render(policy_value_iter, env, 'snap_value_iter.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Policy Iteration(6 epochs): </h5> <img src=\"/snap_policy_iter.gif\"> \n",
    "<h5> Value Iteration(6 epochs): </h5> <img src=\"/snap_value_iter.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "So, there is another policy (right, right, down, down, down, right) with same reward as current optimal policy. Both of algorithms did not found this policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lake:\n",
    "    \n",
    "    def __init__(self, eta=0.8) -> None:\n",
    "        # Initialize the Lake environment\n",
    "        self.num_states = 25\n",
    "        self.num_actions = 4\n",
    "        self.eta = eta\n",
    "\n",
    "        # Define the state and action spaces\n",
    "        self.observation_space = np.arange(self.num_states)\n",
    "        self.action_space = np.array([0, 1, 2, 3])  # 0->left, 1->down, 2->right, 3->up\n",
    "\n",
    "        # Define special states and rewards\n",
    "        self.start = 15\n",
    "        self.walls = np.array([6, 11, 13])\n",
    "        self.g1 = 12  # state with +1 exit\n",
    "        self.g2 = 14  # state with +10 exit\n",
    "\n",
    "        self.is_reward_set = False\n",
    "        self.R = np.zeros(25)  # Reward values for states\n",
    "\n",
    "        self.is_P_set = False\n",
    "        self.P = {}  # Transition probabilities\n",
    "\n",
    "        # Initialize rewards and transition probabilities\n",
    "        self.set_rewards()\n",
    "        self.set_prob()\n",
    "\n",
    "    def set_rewards(self):\n",
    "        # Set the rewards for different states\n",
    "        if not self.is_reward_set:\n",
    "            self.R[self.g1] = 1\n",
    "            self.R[self.g2] = 10\n",
    "\n",
    "            self.R[20:25] = -10  # Negative rewards\n",
    "\n",
    "            self.is_reward_set = True\n",
    "\n",
    "    def set_prob(self):\n",
    "        # Set transition probabilities\n",
    "        if not self.is_P_set:\n",
    "            self.set_rewards()\n",
    "\n",
    "            actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
    "\n",
    "            for i in range(5):\n",
    "                for j in range(5):\n",
    "                    curr_state = 5 * i + j\n",
    "                    self.P[curr_state] = {}\n",
    "\n",
    "                    for intended_a in self.action_space:\n",
    "                        temp = []\n",
    "                        for oth_a in self.action_space:\n",
    "                            next_i = i + actions[oth_a][0]\n",
    "                            next_j = j + actions[oth_a][1]\n",
    "                            prob = 0.0\n",
    "                            next_state = curr_state\n",
    "\n",
    "                            if intended_a == oth_a:\n",
    "                                prob = self.eta  # Transition probability for intended action\n",
    "                            else:\n",
    "                                prob = (1 - self.eta) / 3  # Transition probability for other actions\n",
    "\n",
    "                            if next_i < 0 or next_j < 0 or next_i > 4 or next_j > 4 or 5 * next_i + next_j in self.walls:\n",
    "                                temp.append((prob, next_state, self.R[next_state]))  # Transition tuple\n",
    "                            else:\n",
    "                                next_state = 5 * next_i + next_j  # Update to the new state\n",
    "                                temp.append((prob, next_state, self.R[next_state]))  # Transition tuple\n",
    "\n",
    "                        self.P[curr_state][intended_a] = temp  # Store transitions for intended action\n",
    "\n",
    "        self.is_P_set = True\n",
    "\n",
    "    def print_grid(self):\n",
    "        # Print the grid representation of the environment\n",
    "        print('''\\nB-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)''')\n",
    "        for s in range(20):\n",
    "            i = s // 5\n",
    "            j = s - i * 5\n",
    "            if s in self.walls:\n",
    "                print('W', end=' ')\n",
    "            elif s == self.start:\n",
    "                print('S', end=' ')\n",
    "            elif s == self.g1:\n",
    "                print('g', end=' ')\n",
    "            elif s == self.g2:\n",
    "                print('G', end=' ')\n",
    "            else:\n",
    "                print('B', end=' ')\n",
    "            \n",
    "            if j % 5 == 4:\n",
    "                print('')\n",
    "        \n",
    "        for s in range(5):\n",
    "            print('R', end=' ')\n",
    "\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "Rewards: \n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   0.  10.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [-10. -10. -10. -10. -10.]]\n",
      "\n",
      "Transition Probabilities of state 0 with action 1(Down) is: [(0.06666666666666665, 0, 0.0), (0.8, 5, 0.0), (0.06666666666666665, 1, 0.0), (0.06666666666666665, 0, 0.0)] \n"
     ]
    }
   ],
   "source": [
    "env = lake()\n",
    "env.set_prob()\n",
    "\n",
    "env.print_grid()\n",
    "print(f'Rewards: \\n{env.R.reshape(5,5)}\\n')\n",
    "print(f'Transition Probabilities of state 0 with action 1(Down) is: {env.P[0][1]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.95):\n",
    "    # Get the number of states and actions from the environment\n",
    "    num_states = env.num_states\n",
    "    num_actions = env.num_actions\n",
    "\n",
    "    policy = np.zeros(num_states)\n",
    "\n",
    "    for s in range(num_states):\n",
    "        q_values = np.zeros(num_actions)\n",
    "\n",
    "        for a in range(num_actions):\n",
    "            for (prob, next_state, reward) in env.P[s][a]:\n",
    "                # Calculate the Q-value for each action using the Bellman equation\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=0.95, theta=1e-6):\n",
    "    # Get the number of states from the environment\n",
    "    num_states = env.num_states\n",
    "\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(num_states):\n",
    "            v = V[s]\n",
    "            new_v = 0\n",
    "            a = policy[s]\n",
    "\n",
    "            for (prob, next_state, reward) in env.P[s][a]:\n",
    "                # Calculate the new value using the Bellman equation and the policy\n",
    "                new_v += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "            V[s] = new_v\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "\n",
    "        if delta < theta:\n",
    "            # Terminate if the value function has converged\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "def policy_iteration(env, gamma=0.95):\n",
    "    # Get the number of states and actions from the environment\n",
    "    num_states = env.num_states\n",
    "    num_actions = env.num_actions\n",
    "\n",
    "    V = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        V = policy_evaluation(env, policy, gamma)\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "\n",
    "        if (np.array_equal(policy, new_policy)):\n",
    "            print(f'Breaking at epoch: {epoch}')\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 4\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[117.57684128 125.91770482 134.89683741 143.91973407 153.41271524]\n",
      " [110.31031508 131.26989969 142.0987266  152.64771251 164.29673988]\n",
      " [110.19909429 125.51836016 133.84337159 162.24563332 166.45717548]\n",
      " [117.43976733 127.39936463 138.33967059 150.42755878 163.18907844]\n",
      " [108.40720658 117.18006418 127.41084238 138.66213816 149.77744457]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 2. 2. 1.]\n",
      " [3. 2. 2. 2. 1.]\n",
      " [1. 2. 3. 2. 0.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "V, policy = policy_iteration(env)\n",
    "\n",
    "env.print_grid()\n",
    "\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for different values of $\\eta$ (Above is for $\\eta=0.8$ (default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\eta=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 5\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[154.75616923 162.90123173 171.47498173 180.49998173 189.99998173]\n",
      " [147.01836077 171.47498173 180.49998173 189.99998173 199.99998173]\n",
      " [154.75617015 163.90123265 171.47498265 199.99998173 199.99998173]\n",
      " [162.90123265 171.47498265 180.49998265 189.99998265 199.99998265]\n",
      " [154.75617101 162.90123351 171.47498351 180.49998351 189.99998351]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 1. 1. 1.]\n",
      " [1. 2. 2. 2. 1.]\n",
      " [1. 2. 1. 2. 0.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=1)\n",
    "V, policy = policy_iteration(env)\n",
    "env.print_grid()\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\eta = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 2\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[39.8169965  45.82636322 52.65331472 59.53687285 66.4762782 ]\n",
      " [34.36269039 46.38256047 55.45695889 64.40333711 75.78691773]\n",
      " [28.85115074 38.04727967 47.88156502 68.9607538  80.13624677]\n",
      " [21.42742409 26.59759248 38.17061698 50.21638493 67.68650006]\n",
      " [11.27894901 15.9742469  25.42295601 36.23593299 48.12942929]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 2. 2. 1.]\n",
      " [3. 2. 2. 2. 1.]\n",
      " [3. 2. 3. 2. 0.]\n",
      " [3. 2. 2. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=0.5)\n",
    "V, policy = policy_iteration(env)\n",
    "env.print_grid()\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\eta=0.33$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 2\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[  0.61635014   3.93273435   6.80438429  10.55306395  13.96368385]\n",
      " [ -4.13873155   2.22881733   5.74048451  11.74914317  18.49894214]\n",
      " [-12.14024891 -12.38388043  -3.90169104   7.07009343  18.56037658]\n",
      " [-26.82440614 -29.63471212 -21.17383747 -18.55562968  -3.08430858]\n",
      " [-41.50681389 -41.40442277 -36.87167374 -32.64460726 -25.34978801]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 2. 2. 1.]\n",
      " [3. 2. 2. 2. 1.]\n",
      " [3. 2. 3. 2. 0.]\n",
      " [3. 2. 3. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=0.33)\n",
    "V, policy = policy_iteration(env)\n",
    "env.print_grid()\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\eta=0.25$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 0\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[-16.03692606 -12.07698549 -10.65957089  -7.45510565  -5.15209467]\n",
      " [-23.37306545 -15.61762419 -14.69074418  -8.12314968  -3.93373751]\n",
      " [-35.62985299 -38.46698709 -29.43493414 -21.05018279  -9.88044068]\n",
      " [-55.38766481 -59.48712114 -52.48121776 -52.77277288 -38.95987033]\n",
      " [-72.18026787 -72.58977174 -69.80503746 -67.46097424 -62.42847671]]\n",
      "\n",
      "Policy:\n",
      " [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=0.25)\n",
    "V, policy = policy_iteration(env)\n",
    "env.print_grid()\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For $\\eta=0.8, 1$ algorithm suggesting shortest path from starting state to exit with +10 reward\n",
    "* For $\\eta=0.5, 0.33$ algorithm suggesting longer path from starting state to exit with +10 reward\n",
    "* For $\\eta=0.25$ algorithm suggesting to stay in current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for different values of discount factor, $\\gamma$ (Above is for $\\gamma=0.95$ (default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\gamma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 6\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[4711.3584145  4723.98106404 4736.17101462 4747.49053945 4758.6586508 ]\n",
      " [4703.09028861 4732.65447155 4746.1834246  4758.44948544 4770.72738986]\n",
      " [4713.53934123 4730.75985477 4741.65986177 4769.32490101 4773.09876399]\n",
      " [4725.54815873 4738.83704156 4752.25429465 4765.68884616 4779.12689547]\n",
      " [4721.74713967 4734.77564461 4748.16120866 4761.52356323 4773.99634511]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 2. 2. 1.]\n",
      " [1. 2. 2. 2. 1.]\n",
      " [1. 2. 1. 2. 0.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=0.8)\n",
    "V, policy = policy_iteration(env, gamma=1)\n",
    "\n",
    "env.print_grid()\n",
    "\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for $\\gamma=0.33$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking at epoch: 3\n",
      "\n",
      "B-Blank, S-Start, g-Exit with +1 reward, G-Exit with +10 reward, W-Wall, R-Negative states(-10)\n",
      "B B B B B \n",
      "B W B B B \n",
      "B W g W G \n",
      "S B B B B \n",
      "R R R R R \n",
      "\n",
      "\n",
      "Value Function:\n",
      " [[ 2.77100143e-02  9.97356793e-02  3.58855032e-01  9.65884153e-01\n",
      "   3.27884173e+00]\n",
      " [ 7.29984753e-03  3.37291797e-01  1.24059327e+00  3.23299157e+00\n",
      "   1.17928916e+01]\n",
      " [-1.53085128e-02  1.14535762e+00  1.26621067e+00  1.16058905e+01\n",
      "   1.28416203e+01]\n",
      " [-7.52822600e-01 -6.26502720e-01  4.59759747e-01  2.27516835e+00\n",
      "   1.10366710e+01]\n",
      " [-2.35315128e+00 -2.31215735e+00 -2.00562923e+00 -1.45520450e+00\n",
      "   9.22245432e-01]]\n",
      "\n",
      "Policy:\n",
      " [[2. 2. 1. 2. 1.]\n",
      " [3. 2. 1. 2. 1.]\n",
      " [3. 2. 0. 2. 0.]\n",
      " [3. 2. 3. 2. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "env = lake(eta=0.8)\n",
    "V, policy = policy_iteration(env, gamma=0.33)\n",
    "\n",
    "env.print_grid()\n",
    "\n",
    "print('\\nValue Function:\\n', V.reshape(5,5))\n",
    "print('\\nPolicy:\\n', policy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For $\\gamma=0.95,1$, its algorithm is selecting shortest path to exit state with +10 reward\n",
    "* For $\\gamma=0.33$, its algorithm is selecting longer path to exit state with +1 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) (iii)\n",
    "\n",
    "Scaling discount factor $\\gamma$ from 0.33 to 0.95 or 1 is changing it's policy. So, scaling $\\gamma$ by constant factor may change optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
